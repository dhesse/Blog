% Group Conflicts in Africa, Spark Edition

In the [last][rgrouping] [two][last] posts I did some exploration of a
fascinating data set published by the *Armed Conflict Location & Event
Data Project* ([ACLED][acled]). You can find the code I used on
[github][blog-code-github]. The ACLED data lists incidents in armed
conflicts all over Africa and some countries in South and Southeast
Asia, since 1997 and in great detail.

At first, we used [R][R] and [ggmap][ggmap] to plot the recorded
incidents on a map. These can be *protests* or *riots*, but also
outbursts of violence with many *fatalities*. To get a better overview
we then used [hierarchical clustering][hclust-wiki] to group those
conflicts temporally and spatially. Something like this might be of
value for a NGO or a news outlet in need a system that automatically
assigns a reported incident to a conflict without human interference.

While the clustering worked well, we had to play a little trick that
you might have noticed if you read the last post. In our code, we had
the following transformations applied to our data.

<insert kind='source' file='clustering.R' lines='7-12'>

There, in line 2 you have it. We only considered incidents with more
than 10 fatalities. This might make sense if one is only interested in
*armed* conflicts, but we did it only because of limited memory on the
computer the code ran on. Time to bring out the *big(ish) guns*.

[Apache Spark][spark] is currently my favorite big data tool. You can
run it on your laptop, on a small cluster, or on a warehouse. All with
the same code, on the same interface, with the same language (today,
we'll do [Python][python]). And it's *lightning fast*. The first step
is to read the data into Spark. The ACLED set has some extra newlines
in the description column, which is why we have to jump through some
hoops (`csv` files usually use newlines as separator between
records).

<insert kind='nbcell' file='Cluster.ipynb' cell='1'>

Now we're ready to cluster. Let's look at the clustering methods Spark
supports. In the [documentation][spark-clustering] of MLLib, Spark's
preferred machine learning library, we find a number of choices. For
simplicity, let's stick to [k-means][k-means]. The advantage of
k-means is that it's a very *simple* but *fast* algorithm, but we have
to specify the number of clusters we want to find. This can be bad for
some applications, but since this is an educational post, let's go for
it. Remember that we have to *scale* our inputs as discussed
[last time][rgrouping], luckily Spark has a class that does just that
for us.

<insert kind='nbcell' file='Cluster.ipynb' cell='2'>

To plot the data on a map, we use the amazing [basemap][basemap]
package in python. You can check the [notebook][notebook] I used for
this post if you are interested in the gory details. The result looks
like this.

![](https://dataadventuresdotcom.files.wordpress.com/2016/04/africa15.png)

Again, the clustering looks sensible. A news outlet that wants to
automatically assign conflict identifiers to incident reports might
just use a model like this. Next time, I would like to dive a bit
deeper into the data set and explore the individual clusters.

[last]: http://data-adventures.com/2016/04/23/armed-conflicts-in-africa-illustrated-in-r
[rgrouping]: https://data-adventures.com/2016/04/24/data-science-for-a-cause-grouping-conflicts-in-africa/
[acled]: http://www.acleddata.com/
[blog-code-github]: https://github.com/dhesse/BlogCode
[spark-clustering]: http://spark.apache.org/docs/latest/mllib-clustering.html
[notebook]: https://github.com/dhesse/BlogCode/blob/master/conflicts/Cluster.ipynb
[hclust-wiki]: https://en.wikipedia.org/wiki/Hierarchical_clustering
[R]: https://r-project.org
[ggmap]: https://cran.r-project.org/web/packages/ggmap/index.html
[spark]: http://spark.apache.org/
[python]: https://www.python.org/
[k-means]: https://en.wikipedia.org/wiki/K-means_clustering
[basemap]: http://matplotlib.org/basemap/
