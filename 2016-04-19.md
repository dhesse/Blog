% A Data Scientist's Toolbox, part 2: Testing your code.

[In the last post][tool1], I talked about the usefulness of
[REPLs][repl] which indeed is hard to overstate. Exploratory data
analysis would be a lot of hassle without the read-evaluate-print
loop. We had a closer look at [Jupyter][jupyter] in particular and
first attempts at analyzing the data from the post
[on food and inflation][fi] can be found [on github][toolrepo].

So REPLs are great tools. Really great. I mean, for exploratory
analysis. Most of it, that is. *Let me explain*. REPLs have one major
drawback. Code in e.g. a Jupyter notebook is *very hard to test* and
the thing about code that's hard to test is that it usually doesn't
get tested. *At all*. It's just in our nature to be lazy, so if
something is a hassle you usually just don't do it, unless it's
strictly required. And clearly your code will execute (after you
fiddle with it until it *seems* to work alright, at which point you
save and slowly walk away from your computer) just fine without
tests. But do you *know* it's doing the right thing?

Let's take a minute to talk about test driven development (or
[TDD][tdd]). Some argue that you don't need unit testing, for various
reasons. Like it slows you down when coding. Or it gives you a *false*
sense of security. And so on. All the arguments I've heard against TDD
have one thing in common though. They're *wrong*. I tend to agree
[with Uncle Bob][tdddebate] on the matter. If you are unfamiliar with
unit testing, I recommend you have a look at some
[introductory material][tddrules].

Now we're no software developers but data scientists. I don't demand
that you unit test every last line of your analysis. But I do believe
it is a good idea to break out what you can from a notebook and write
some tests for it. Why? *Code reuse*. If you have that neat function
in a separate file with some tests going on, you'll be more likely to
re-use it. Why? Because you trust it. Because it's tested. Let's have
a look.

Last time, we had a neat little function that would normalize a
series.

<insert kind="nbcell"
file="Notebook.ipynb"
cell="5">

Any function you write might be a good candidate to be but in a
separate file. Even tough this specific one might not be the best
example, but let's break it out and write a test. The
[unittest][utest] package is one of the obvious and popular choices
for this task.

<insert kind="source"
file="normalize_basic.py"
lines="9-19"">

Now, while writing the test, you should notice a few strange
things. What happens if there is no year 2000 in our data frame? What
happens if we have it multiple times? This would break our code. Just
a few more lines and two more tests make the function much more
reliable, and by proxy you much more likely to use it in a future
analysis.

<insert kind="source"
file="normalize.py"
lines="10-35">

Now, let's do the same with the reading and pre-processing of our
data.

<insert kind="source"
file="read_data.py"
lines="4-62">

Our notebook becomes much more readable as well.

<insert kind="gist"
user="dhesse"
id="9dc6c01601e4f0d337f48a48786ac36c">

So if you have a useful function in your EDA, be a good data
scientist, put it in a separate file, test, refactor, and save
yourself some time in the future.

[tool1]: http://data-adventures.com/2016/04/18/a-data-scientists-toolbox-part-1-repls/
[repl]: https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop
[jupyter]: http://jupyter.org/
[fi]: http://data-adventures.com/2016/04/17/food-and-inflation/
[toolrepo]: http://github.com/dhesse/ToolsBlog
[tdd]: https://en.wikipedia.org/wiki/Test-driven_development
[tdddebate]: http://butunclebob.com/ArticleS.UncleBob.UntestedCodeDarkMatter
[utest]: https://docs.python.org/2/library/unittest.html
[tddrules]: http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd
